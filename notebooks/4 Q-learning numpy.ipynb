{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning numpy\n",
    "\n",
    "Our agent needs to cross a lake, represented by a square divided with a 4x4 grid. At every step he needs to take an action, he can go up, down, left or right. If it's not possible to go to the chosen direction our agent will simply not move.\n",
    "\n",
    "Each grid element can be frozen, in that case our agent will be able to move, it can he a hole, in that case the episodes end and we don't get the reward or it can be the final state, the goal. In that case the agent will receive a positive reward.\n",
    "\n",
    "We want to use q-learning to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T20:06:08.232845Z",
     "start_time": "2019-04-30T20:06:08.217471Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# Meta parameters for the RL agent\n",
    "alpha = 0.01\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 1.0\n",
    "verbose = False\n",
    "\n",
    "# Define types of algorithms\n",
    "EPSILON_GREEDY = \"epsilon_greedy\"\n",
    "SOFTMAX = \"softmax\"\n",
    "GREEDY = \"greedy\"\n",
    "\n",
    "# Choose methods for learning and exploration\n",
    "rl_algorithm = 'QLEARNING'\n",
    "explore_method = EPSILON_GREEDY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T20:06:08.510162Z",
     "start_time": "2019-04-30T20:06:08.405937Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax values. \n",
    "    Returns a vector with.\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum()\n",
    "\n",
    "\n",
    "# Act with softmax\n",
    "def take_action_using_softmax(s, q):\n",
    "    prob_a = softmax(q[s, :])\n",
    "    cumsum_a = np.cumsum(prob_a)\n",
    "    return np.where(np.random.rand() < cumsum_a)[0][0]\n",
    "\n",
    "\n",
    "# Act with epsilon greedy\n",
    "def take_action_using_epsilon_greedy(s, q):\n",
    "    a = np.argmax(q[s, :])\n",
    "    if np.random.rand() < epsilon:\n",
    "        a = np.random.randint(q.shape[1])\n",
    "    return a\n",
    "\n",
    "\n",
    "def q_learning_update(q, s, a, r, s_prime):\n",
    "    # TD Update \n",
    "    return\n",
    "\n",
    "# Evaluate a policy on n runs\n",
    "def evaluate_policy(q, env, n, h, explore_type):\n",
    "    success_rate = 0.0\n",
    "    mean_return = 0.0\n",
    "\n",
    "    for i in range(n):\n",
    "        discounted_return = 0.0\n",
    "        s = env.reset()\n",
    "\n",
    "        for step in range(h):\n",
    "            if explore_type == GREEDY:\n",
    "                s, r, done, info = env.step(np.argmax(q[s, :]))\n",
    "            elif explore_type == EPSILON_GREEDY:\n",
    "                s, r, done, info = env.step(\n",
    "                    take_action_using_epsilon_greedy(s, q))\n",
    "            elif explore_type == SOFTMAX:\n",
    "                s, r, done, info = env.step(take_action_using_softmax(s, q))\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Wrong Explore Method in evaluation:\".format(explore_type))\n",
    "\n",
    "            discounted_return += np.power(gamma, step) * r\n",
    "\n",
    "            if done:\n",
    "                success_rate += float(r) / n\n",
    "                mean_return += float(discounted_return) / n\n",
    "                break\n",
    "\n",
    "    return success_rate, mean_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T20:06:08.585807Z",
     "start_time": "2019-04-30T20:06:08.558287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode  3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose environment\n",
    "env_name = 'FrozenLake-v0'\n",
    "\n",
    "# Create Environment\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Get State-Action space size\n",
    "n_a = 4\n",
    "n_s = 4\n",
    "\n",
    "# Experimental setup\n",
    "n_episode = 3000\n",
    "print(\"n_episode \", n_episode)\n",
    "max_horizon = 100\n",
    "eval_steps = 10\n",
    "\n",
    "# Monitoring performance\n",
    "window = deque(maxlen=100)\n",
    "\n",
    "greedy_success_rate_monitor = np.zeros([n_episode, 1])\n",
    "greedy_discounted_return_monitor = np.zeros([n_episode, 1])\n",
    "\n",
    "behaviour_success_rate_monitor = np.zeros([n_episode, 1])\n",
    "behaviour_discounted_return_monitor = np.zeros([n_episode, 1])\n",
    "\n",
    "# Init Q-table\n",
    "q_table = np.zeros([n_s, n_a])\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T20:06:19.031059Z",
     "start_time": "2019-04-30T20:06:08.708407Z"
    }
   },
   "outputs": [],
   "source": [
    "verbose = False\n",
    "# Train for n_episode\n",
    "\n",
    "for i_episode in range(n_episode):\n",
    "\n",
    "    # Reset the expisode's cumulative reward\n",
    "    total_return = 0.0\n",
    "\n",
    "    # Start fresh with a new episode state\n",
    "    s = env.reset()\n",
    "\n",
    "    # Select the first action in this episode\n",
    "\n",
    "    a = take_action_using_epsilon_greedy(s, q_table)\n",
    "\n",
    "    for i_step in range(max_horizon):\n",
    "\n",
    "        # Take action and receive the enviroment's feedback\n",
    "        s_prime, r, done, info = env.step(a)\n",
    "\n",
    "        total_return += np.power(gamma, i_step) * r\n",
    "\n",
    "        # Select an action\n",
    "        if explore_method == SOFTMAX:\n",
    "            a_prime = take_action_using_softmax(s_prime, q_table)\n",
    "        elif explore_method == EPSILON_GREEDY:\n",
    "            a_prime = take_action_using_epsilon_greedy(s_prime, q_table)\n",
    "        else:\n",
    "            raise ValueError(\"Wrong Explore Method:\".format(explore_method))\n",
    "\n",
    "        # Update a Q value table\n",
    "        q_table[s, a] = q_learning_update(q_table, s, a, r, s_prime)\n",
    "\n",
    "\n",
    "        # Transition to new state\n",
    "        s = s_prime\n",
    "        a = a_prime\n",
    "\n",
    "        if done:\n",
    "            window.append(r)\n",
    "            last_100 = window.count(1)\n",
    "\n",
    "            greedy_success_rate_monitor[i_episode - 1, 0], greedy_discounted_return_monitor[\n",
    "                i_episode - 1, 0] = evaluate_policy(q_table, env, eval_steps, max_horizon, GREEDY)\n",
    "            behaviour_success_rate_monitor[i_episode - 1, 0], behaviour_discounted_return_monitor[\n",
    "                i_episode - 1, 0] = evaluate_policy(q_table, env, eval_steps, max_horizon, explore_method)\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"Episode: {0}\\t Num_Steps: {1:>4}\\tTotal_Return: {2:>5.2f}\\tFinal_Reward: {3}\\tEpsilon: {4:.3f}\\tSuccess Rate: {5:.3f}\\tLast_100: {6}\".format(\n",
    "                        i_episode, i_step, total_return, r, epsilon, greedy_success_rate_monitor[i_episode - 1, 0],\n",
    "                        last_100))\n",
    "\n",
    "            break\n",
    "\n",
    "    # Schedule for epsilon\n",
    "    epsilon = epsilon * epsilon_decay\n",
    "\n",
    "\n",
    "env.render()\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(range(0, n_episode, 10), greedy_success_rate_monitor[0::10, 0])\n",
    "plt.title(\"Greedy policy with {0} and {1}\".format(rl_algorithm, explore_method))\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Success Rate\")\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(range(0, n_episode, 10), behaviour_success_rate_monitor[0::10, 0])\n",
    "plt.title(\"Behaviour policy with {0} and {1}\".format(rl_algorithm, explore_method))\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Success Rate\")\n",
    "plt.show()\n",
    "\n",
    "# Show an episod\n",
    "\n",
    "for i_step in range(max_horizon):\n",
    "    env.render()\n",
    "    a = np.argmax(q_table[s, :])\n",
    "    s, r, done, info = env.step(a)\n",
    "    total_return += np.power(gamma, i_step) * r\n",
    "\n",
    "    if done:\n",
    "        print(\"Episode: {0}\\t Num_Steps: {1:>4}\\tTotal_Return: {2:>5.2f}\\tFinal_Reward: {3}\".format(1, i_step,\n",
    "                                                                                                    total_return,\n",
    "                                                                                                    r))\n",
    "        break\n",
    "\n",
    "# Show Policy\n",
    "\n",
    "for s in range(n_s):\n",
    "    actions = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
    "    print(actions[np.argmax(q_table[s, :])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
